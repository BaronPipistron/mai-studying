{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZQFmgHTvz1b",
    "outputId": "c66f21f5-5aad-4c84-8167-7b7798273339"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2025-05-21 07:27:56--  https://github.com/tonsoleils/AI/raw/main/lab3/dataset/bbc-news-summary.zip\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/tonsoleils/AI/main/lab3/dataset/bbc-news-summary.zip [following]\n",
      "--2025-05-21 07:27:57--  https://raw.githubusercontent.com/tonsoleils/AI/main/lab3/dataset/bbc-news-summary.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4558162 (4.3M) [application/zip]\n",
      "Saving to: ‚Äòbbc-news-summary.zip‚Äô\n",
      "\n",
      "bbc-news-summary.zi 100%[===================>]   4.35M  --.-KB/s    in 0.05s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2025-05-21 07:27:57 (81.6 MB/s) - ‚Äòbbc-news-summary.zip‚Äô saved [4558162/4558162]\n",
      "\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "ü§ñ  gpu mem : 102.9/15095.1 mb\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "!wget -N https://github.com/tonsoleils/AI/raw/main/lab3/dataset/bbc-news-summary.zip\n",
    "!unzip -q -o bbc-news-summary.zip -d ./\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "%pip install -q sentencepiece\n",
    "%pip install -q transformers  datasets\n",
    "%pip install -q accelerate\n",
    "%pip install -q deepspeed mpi4py\n",
    "%pip install -q pynvml\n",
    "%pip install -q wandb\n",
    "%pip install --upgrade transformers\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class Profiler():\n",
    "\n",
    "    def __init__(self,) -> None:\n",
    "        pass\n",
    "\n",
    "    def gpu_mem(self):\n",
    "        mem = torch.cuda.mem_get_info()\n",
    "        mb = list(map(lambda x:x/pow(2,20),mem))\n",
    "        total = mb[1]\n",
    "        used = mb[1]-mb[0]\n",
    "        return used,total\n",
    "\n",
    "    def gpu_mem_info(self,title = ''):\n",
    "        used,total = self.gpu_mem()\n",
    "        print(f'ü§ñ {title} gpu mem : {used:.1f}/{total:.1f} mb')\n",
    "\n",
    "    def one_step_report(self,batch, model, optimizer, do_backward = True,device = torch.device('cpu'),print_loss = False,deepspeed = False):\n",
    "\n",
    "        report_df = pd.DataFrame(columns=['used_mem','delta_mem','delta_time'])\n",
    "\n",
    "        delta_time =[0]\n",
    "        used_mem = [self.gpu_mem()[0]]\n",
    "\n",
    "        self.gpu_mem_info('begin')\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        ids = batch['input_ids'].to(device,dtype=torch.long)\n",
    "        labels = batch['labels'].to(device,dtype=torch.long)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "\n",
    "        outputs = model(input_ids = ids,labels = labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        forward_time = time.time()\n",
    "        delta_time.append(-start_time + forward_time)\n",
    "\n",
    "        used_mem.append(self.gpu_mem()[0])\n",
    "        self.gpu_mem_info(f'{delta_time[-1]:.3f}s forward')\n",
    "        if do_backward:\n",
    "            optimizer.zero_grad()\n",
    "            if deepspeed:\n",
    "                model.backward(loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            backward_time = time.time()\n",
    "            delta_time.append(-forward_time + backward_time)\n",
    "            used_mem.append( self.gpu_mem()[0])\n",
    "            self.gpu_mem_info(f'{delta_time[-1]:.3f}s backward')\n",
    "\n",
    "            if deepspeed:\n",
    "                model.step()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            optimizer_step_time = time.time()\n",
    "            delta_time.append(-backward_time + optimizer_step_time)\n",
    "            used_mem.append( self.gpu_mem()[0])\n",
    "            self.gpu_mem_info(f'{delta_time[-1]:.3f}s optimizer_step')\n",
    "\n",
    "        if (print_loss):\n",
    "            print('loss',loss)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        used_mem.append( self.gpu_mem()[0])\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        delta_time.append(end_time - optimizer_step_time)\n",
    "\n",
    "        report_df.loc[:,'used_mem'] = pd.Series(used_mem)\n",
    "        report_df.loc[:,'delta_time'] = pd.Series(delta_time)\n",
    "        indexes = ['begin','forward','backward','optim_step','end']\n",
    "        report_df.index = indexes\n",
    "\n",
    "        report_df['delta_mem'] =  report_df['used_mem']- report_df.loc['begin','used_mem']\n",
    "\n",
    "        report_df.loc['total'] = [self.gpu_mem()[1],0,end_time-start_time]\n",
    "        report_df['delta_time'] = report_df['delta_time'].map(lambda t : round(t,3))\n",
    "\n",
    "        return report_df\n",
    "\n",
    "prof = Profiler()\n",
    "prof.gpu_mem()\n",
    "prof.gpu_mem_info()\n",
    "\n",
    "def gpu_mem():\n",
    "    mem = torch.cuda.mem_get_info()\n",
    "    mb = list(map(lambda x:x/pow(2,20),mem))\n",
    "    total = mb[1]\n",
    "    used = mb[1]-mb[0]\n",
    "    return used,total\n",
    "\n",
    "def gpu_mem_info(title = ''):\n",
    "    used,total = gpu_mem()\n",
    "    print(f'ü§ñ {title} gpu mem : {used:.1f}/{total:.1f} mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a77n62IqFRI5",
    "outputId": "6f715435-dfa9-4683-faaa-45525e036280"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bbc-news-summary      cached_lm_GPT2Tokenizer_512_ds.txt       ds.txt\n",
      "bbc-news-summary.zip  cached_lm_GPT2Tokenizer_512_ds.txt.lock  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St0zCzK4ySMc"
   },
   "source": "## –í—ã–±–µ—Ä–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏ –º–æ–¥–µ–ª—å"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MDZXGPlvn59"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'ai-forever/rugpt3small_based_on_gpt2' #@param ['ai-forever/rugpt3small_based_on_gpt2', 'ai-forever/rugpt3medium_based_on_gpt2','ai-forever/rugpt3large_based_on_gpt2', 'gpt2-large']\n",
    "DATASET_PATH1 = \"Summaries/tech\" # @param [\"News Articles/business\", \"News Articles/entertainment\", \"News Articles/politics\", \"News Articles/sport\", \"News Articles/tech\", \"Summaries/business\", \"Summaries/entertainment\", \"Summaries/politics\", \"Summaries/sport\", \"Summaries/tech\"]\n",
    "DATASET_PATH = \"bbc-news-summary/\" + DATASET_PATH1\n",
    "\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def read_text(paths):\n",
    "    documents = []\n",
    "    for root, dirs, files in os.walk(paths):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    lines = f.readlines()\n",
    "                    content = ''.join(lines)\n",
    "                    documents.append(content)\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Error reading file: {file_path}\")\n",
    "\n",
    "    return '\\n'.join(documents)\n",
    "\n",
    "spam = ['\\ufeff', '\\t', '\\u2060', '¬¶', '¬´', '¬ª', '\\n', \"\\'\", \"\\\"\"]\n",
    "\n",
    "def clear_vocab(text: str) -> set:\n",
    "    extra_symbols = ['<EOS>', '<BOS>', '<UNK>']\n",
    "    vocab = set()\n",
    "    for char in text:\n",
    "        if char not in vocab:\n",
    "            vocab.add(char)\n",
    "    for char in extra_symbols:\n",
    "        vocab.add(char)\n",
    "    return vocab\n",
    "\n",
    "def clear_text(symbols: list, text: str) -> None:\n",
    "    new_sentences = []\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        new_sentence = []\n",
    "        for symbol in sentence:\n",
    "            if symbol not in symbols:\n",
    "                new_sentence.append(symbol)\n",
    "        if len(new_sentence) >= 20:\n",
    "            new_sentences.append(''.join(new_sentence))\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFLJx5TJz1X3"
   },
   "source": [
    "## –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVU_Tf-hz2f6",
    "outputId": "06beb215-7f2c-49e3-d38f-474af1ce2f1f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2025-05-21 07:29:13,443] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Working on cuda\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import deepspeed\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Working on {DEVICE}\")\n",
    "\n",
    "model_name_or_path = MODEL_NAME\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aufooHm602s-",
    "outputId": "121b36c1-ddc9-4940-a19d-a73b66d5df64"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The total number of parameters in the model is 125231616\n",
      "ü§ñ  gpu mem : 672.9/15095.1 mb\n"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of parameters in the model is {model.num_parameters()}\")\n",
    "gpu_mem_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkA4jcot2DAZ"
   },
   "source": [
    "## –î–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "mX3v8j4mHXAq",
    "outputId": "9b6b16b6-5aff-47f4-faac-5641389fd97a"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'bbc-news-summary/Summaries/tech'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTnccJIxHIDy"
   },
   "outputs": [],
   "source": [
    "text = read_text(DATASET_PATH).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOK37mflKp5f"
   },
   "outputs": [],
   "source": [
    "ds_name = \"ds.txt\"\n",
    "with open(ds_name, \"w\") as f:\n",
    "  for line in text:\n",
    "    out_str = line + \"\\n\" + \"[EOS]\" + \"\\n\"\n",
    "    f.write(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_M-hiBiLG3s",
    "outputId": "745eef86-092d-43da-fe71-86f5bf6f0b03"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "tokenizer.add_tokens('[EOS]')\n",
    "tokenizer.add_special_tokens({\n",
    "    'eos_token': '[EOS]',\n",
    "    'pad_token': '<pad>'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20oR4sgA0kzI",
    "outputId": "8ce8bcdb-e1af-42c7-8759-9c0573170d3f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(tokenizer=tokenizer,file_path=ds_name,block_size=512)\n",
    "train_dataset, eval_dataset = train_test_split(train_dataset,test_size = 0.1,random_state = 42)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mT9bDNn-w6rU",
    "outputId": "2d7aaf28-cb4a-4a64-e18b-7651450c9a7c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤): 50257\n",
      "–°–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤: [' –≤–æ–ª–æ—Å', ' –¥–æ—Ü–µ–Ω—Ç', ' –î–µ—Ç–∏', ' –∫–∞—Ä–¥–∏']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤): {tokenizer.vocab_size}\")\n",
    "print(f\"–°–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤: {[tokenizer.decode(id) for id in np.random.choice(range(tokenizer.vocab_size), 4)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2wl7e8t0Ypy",
    "outputId": "4d3be2b6-ebd6-4074-a004-446aa4c42c57"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "–ø–µ—Ä–≤—ã–π –±–∞—Ç—á –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ —Ä–∞–∑–º–µ—Ä–∞ 512\n",
      "tensor([ 1955, 19748,  3810, 10341,  3115, 32448,  3926,   534,  1366,  1284,\n",
      "         3735,  1824,  1132, 37411,  1622, 45115,  2929,   593,  5724,    17,\n",
      "         2692,    87,  3087,  3970, 34898,    23, 23254,    18, 43842,    23,\n",
      "        23254,  2213, 12986,   463,  1172,   974, 32678,   593,  2369, 21892,\n",
      "         1092, 48293, 40631,    87,    18,    51,    74,   463,  3405, 15714,\n",
      "         5959,  5105,  3764,  8218, 34898,    23, 23254,    16, 19670,     9,\n",
      "         2213, 14530, 42476,   710, 15173,     9,   593, 16332,  4796, 27479,\n",
      "         1638,    87,   654, 34898,    23, 23254,  2213, 30814,   407,  8326,\n",
      "        49276, 35707,  1092, 46515,  1284,     6, 12155, 37318,  1075,  1729,\n",
      "         2751,    16, 21944,   593,   463,   969,  1753, 36975,   654,  7825,\n",
      "        24369, 41292,    18,  1955,  7825, 28690, 15267,  6504,  1338, 41183,\n",
      "         9765, 23254,  3135,  4240,  6756, 26084,  1227,  5614,   654,  5614,\n",
      "        23962,   573, 18481, 18493, 28020,    18,   203, 50257,   203,    55,\n",
      "         9221,    70,  2034, 12155,  1622,   463, 43558,   593,   463, 25571,\n",
      "         7983,    87, 10436,  1010, 37411,   710,  1400, 30472,  1227,   463,\n",
      "        10436, 42996, 24456,  1806,  3840,  7867,  4456,   710, 13599,  1417,\n",
      "        25774,    18,    55,  9221,    70,  2034,  7867, 14138, 16583,  1417,\n",
      "        35229,  1302,  3414,  4632,   461,    16, 46637, 21437,    16, 24992,\n",
      "           53,    16,  4341, 27753,   654,  1149, 37655,  7333,    89,  2213,\n",
      "         3075, 15674, 13014,  1417,   463, 25571,  7983,    87, 10436,    18,\n",
      "         1955, 25571,  7983,    87, 10436,  1132, 29071,   710,  3970,  4240,\n",
      "        17181,  1417,   407,   729,   565,   658,  5586,   752,  2718,   444,\n",
      "        16284,   463,  1473, 30365,   786,  6350,    17,  5759,    22,    18,\n",
      "         1955, 25571,  7983,    87, 10436,  1132, 22243, 27447,  1227,   407,\n",
      "          601,  4058,  9214,    30, 21547,  2479,    18, 11853, 10540,    18,\n",
      "         9582, 11317,    18,  3335, 43070,    18,    94,  2148,    18,  1955,\n",
      "        36676,  3135, 16583, 10495,  1052,  4865,  1092,  3360,  1284,   591,\n",
      "        28225,  4867,   710, 19133,  6668,  6560,  9574,   463, 10436,   654,\n",
      "        18481,  3620,  7867,  4456, 19596,  6073,    18,  1955,   601,  4058,\n",
      "           16,  1002,  5242,    82,  8924, 25571,  7983,    87,    16,  1132,\n",
      "        29071,   710,  3970,  8184,    74, 10871,  1099, 38025, 19204, 49511,\n",
      "         6044,  9073,  5598, 35899,  4728, 20329,  8104,    17,  4545,    16,\n",
      "         6787,  1227, 45404,    84, 18278,    16, 15428,   654,  2144, 12577,\n",
      "          573,    16,  1092,  3620,  7867, 14138, 47814,    70,  2034, 12155,\n",
      "         1622,  1824,  1010,  3115,   535,  3039, 12919,   463, 43684, 25571,\n",
      "         7983,    87, 19100,  1010,  3581,   815,  7752,  2138, 43538,   407,\n",
      "        10804,   593, 40153, 10436, 20952,    18,  1955, 10436,  1132,   729,\n",
      "          633,  1823,   444,   974,   657,  1227, 44068,  1622,  8930,  4306,\n",
      "         4917, 48615,  4536, 10281,   654,  6668,   573,  1092,   463,  8326,\n",
      "        33871,   593,   463, 21547,  2479,  7867,  4456,    18,   203, 50257,\n",
      "          203,  1955,   769,  2386, 15973,   514,    16,  2677,  8852, 44383,\n",
      "        35229,  1564, 18884,  9333,  1096,   816,    16,  8852,  9454,  1533,\n",
      "           17,    88,  7577,   601, 10622,  7039, 12413, 28612,  2369, 30309,\n",
      "           16,  3605,  9237,  1284,  3970, 12155,    18,    45,    38,    49,\n",
      "         3135, 12155,  1824,  8852,   535,  2386,   407,  4567,   849,  1096,\n",
      "         1338,   463,  2369,  2148, 33013,  6864,  4023,  3908,    18,  1955,\n",
      "         2369,  2148,  8852,  9454,  1529, 14981, 28859,   593, 42737,  7039,\n",
      "         1032,  1180,    44,    94,    16,   463,   601,  1106, 14651, 12155,\n",
      "           18, 46507,   463, 18884,  9333,  1096,   816,  1132, 37411,   710,\n",
      "         1400,   463,  3351, 25703,    17, 26064,   980, 19173,   710,  9723,\n",
      "          769,  2386,    16,   463,  2369,  2148,  1564,  9237,  1284,  3970,\n",
      "        12155,   463])\n"
     ]
    }
   ],
   "source": [
    "print(f\"–ø–µ—Ä–≤—ã–π –±–∞—Ç—á –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ —Ä–∞–∑–º–µ—Ä–∞ {len(train_dataset[0])}\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P1dVJ0f2oZ4"
   },
   "source": "## –û–±—É—á–µ–Ω–∏–µ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnsIIg7q2yyO"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 4\n",
    "n_epochs = 15\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=n_epochs,\n",
    "    warmup_steps=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    auto_find_batch_size=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yV2rknYD231e"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    # optimizers = (torch.optim.NAdam(model.parameters(),lr=1e-5),None) # Optimizer and lr scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "xB5ydLnF258U",
    "outputId": "cc4b6c0e-b140-4862-e825-3e882e679df8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 08:20, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "trainer_log = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-3jE19d33S0"
   },
   "source": [
    "## –ì–µ–Ω–µ—Ä–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T6piivT334wF",
    "outputId": "40fa8936-b2f4-4d7c-a1bd-6122a938ff9f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "The other common type of ink in elections is indelible visible ink, which is so popular that the parties are forced to use.\"It's the first time I had been seeking the performer to be aware of the ink,\" said the campaign manager of independent electronic gaming firm Crysis.More than 40,000 people in the country have been cast in campaigns in recent months, with the goal of spending just ¬£3.6bn in the way it is used to get the electronic music downloads.Few European electronic music players can be used to download or sell on a computer.One method of ink is a video recorder that can pause downloads and record results, which can be used to record results for different electronic music firms and other people using a computer to send video on the screen.Users of electronic gaming software can also download\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "text = \"The other common type of ink in elections is indelible visible ink\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "attention_mask = torch.ones_like(input_ids)  # Set all values to 1 initially\n",
    "pad_token_id = 50256  # EOS token ID\n",
    "\n",
    "if pad_token_id is not None:\n",
    "    attention_mask[input_ids == pad_token_id] = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids,\n",
    "                         do_sample=True,\n",
    "                         num_beams=4,\n",
    "                         temperature=2.5,\n",
    "                         top_p=0.9,\n",
    "                         max_length=200,\n",
    "                         attention_mask=attention_mask\n",
    "                         )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
